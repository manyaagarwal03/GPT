Reference: idS20

Tools for the Diagnosis of ADHD in Children and Adolescents: A Systematic Review

Bradley S. Peterson, MD,<sup>a,b</sup> Joey Trampush, PhD,<sup>b</sup> Morah Brown, MPH,<sup>c</sup> Margaret Maglione, MPP,<sup>c</sup> Maria Bolshakova, PhD,<sup>c</sup> Mary Rozelle, MPH,<sup>c</sup> Jeremy Miles, PhD,<sup>c</sup> Sheila Pakdaman, PhD,<sup>c</sup> Sachi Yagyu, MS,<sup>c</sup> Aneesa Motala, BA,<sup>c</sup>

Susanne Hempel, PhD

CONTEXT: Correct diagnosis is essential for the appropriate clinical management of attention- deficit/hyperactivity disorder (ADHD) in children and adolescents.

OBJECTIVE: This systematic review provides an overview of the available diagnostic tools.

DATA SOURCES: We identified diagnostic accuracy studies in 12 databases published from 1980 through June 2023.

STUDY SELECTION: Any ADHD tool evaluation for the diagnosis of ADHD, requiring a reference standard of a clinical diagnosis by a mental health specialist.

DATA EXTRACTION: Data were abstracted and critically appraised by 1 reviewer and checked by a methodologist. Strength of evidence and applicability assessments followed Evidence-based Practice Center standards.

RESULTS: In total, 231 studies met eligibility criteria. Studies evaluated parental ratings, teacher ratings, youth self-reports, clinician tools, neuropsychological tests, biospecimen, EEG, and neuroimaging. Multiple tools showed promising diagnostic performance, but estimates varied considerably across studies, with a generally low strength of evidence. Performance depended on whether ADHD youth were being differentiated from neurotypically developing children or from clinically referred children.

LIMITATIONS: Studies used different components of available tools and did not report sufficient data for meta-analytic models.

CONCLUSIONS: A valid and reliable diagnosis of ADHD requires the judgment of a clinician who is experienced in the evaluation of youth with and without ADHD, along with the aid of standardized rating scales and input from multiple informants across multiple settings, including parents, teachers, and youth themselves.

Attention-deficit/hyperactivity disorder (ADHD) is one of the most prevalent neurodevelopmental conditions in youth. Its prevalence has remained constant at ~5.3% worldwide over the years, and diagnostic criteria have remained constant when based on rigorous diagnostic procedures. Clinical diagnoses, however, have increased steadily over time, and currently, ~10% of US children receive an ADHD diagnosis. Higher rates of clinical com- pared with research-based diagnoses are because of an increasing clinician recognition of youth who have ADHD symptoms that are functionally impairing but do not fully meet formal diagnostic criteria. The higher diagnostic rates over time in clinical samples also results from youth receiving a diagnosis incorrectly. Some youth, for example, are misdiagnosed as having ADHD when they have symptoms of other disorders that overlap with ADHD symptoms, such as difficulty concentrating, which occurs in many other conditions. Moreover, ADHD is more than twice as likely to be diagnosed in boys than in girls, in lower-income families, and in white compared with nonwhite youth; differences that derive at least in part from diagnostic and cultural biases.

Improving clinical diagnostic accuracy is essential to en- sure that youth who truly have ADHD benefit from receiv- ing treatment without delay. Similarly, youth who do not have ADHD should not be diagnosed since an incorrect diag- nosis risks exposing them to unbeneficial treatments. Clinician judgement alone, however, especially by nonspe- cialist clinicians, is poor in diagnosing ADHD compared with expert, research-grade diagnoses made by mental health clinicians. Accurately diagnosing ADHD is difficult because diagnoses are often made using subjective clinical impressions, and putative diagnostic tools have a confusing, diverse, and poorly described evidence base that is not widely accessible. The availability of valid diagnostic tools would especially help to reduce misdiagnoses from cultural biases and symptom overlap with ADHD.

This review summarizes evidence for the performance of tools for children and adolescents with ADHD. We did not restrict to a set of known diagnostic tools but instead explored the range of available diagnostic tools, including machine-learning assisted and virtual reality-based tools. The review aimed to assess how diagnostic performance varies by clinical setting and patient characteristics.

**METHODS**
The review aims were developed in consultation with the Agency for Healthcare Research and Quality (AHRQ), the Patient-Centered Outcomes Research Institute, the topic nominator American Academy of Pediatrics, key inform- ants, a technical expert panel (TEP), and public input. The TEP reviewed the protocol and advised on key outcomes. Subgroup analyses and key outcomes were prespecified. The review is registered in PROSPERO (CRD42022312656) and the protocol is available on the AHRQ Web site as part of a larger evidence report on ADHD. The systematic re- view followed Methods of the (AHRQ) Evidence-based Practice Center Program.

**Selection Criteria**
Population: age <18 years.

Interventions: any ADHD tool for the diagnosis of ADHD.

Comparators: diagnosis by a mental health specialist, such as a psychologist, psychiatrist, or other provider, who often used published scales or semistructured diag- nostic interviews to ensure a reliable DSM-based diagno- sis of ADHD.

Key outcomes: diagnostic accuracy (eg, sensitivity, spe- cificity, area under the curve).

Setting: any.

Study design: diagnostic accuracy studies.

Other: English language, published from 1980 to June 2023.

**Search Strategy**
We searched PubMed, Embase, PsycINFO, ERIC, and Clin- icalTrials.gov. We identified reviews for reference-mining through PubMed, Cochrane Database of Systematic Re- views, Campbell Collaboration, What Works in Education, PROSPERO, ECRI Guidelines Trust, G-I-N, and ClinicalKey. The peer reviewed strategy is in the Supplemental Appendix. All citations were screened by trained litera- ture reviewers supported by machine learning ([Fig 1]. Two independent reviewers assessed full text studies for eligibility. The TEP reviewed studies to ensure all were captured. Publications reporting on the same participants were consolidated into 1 record.

**Data Extraction**
The data abstraction form included extensive guidance to aid reproducibility and standardization in recording study details, results, risk of bias, and applicability. One reviewer abstracted data and a methodologist checked accuracy and completeness. Data are publicly available in the Systematic Review Data Repository.

**Risk of Bias and Applicability**
We assessed characteristics pertaining to patient selec- tion, index test, reference standard, flow and timing that may have introduced bias, and evaluated applicability of study results, such as whether the test, its conduct, or in- terpretation differed from how the test is used in clinical practice.

**Data Synthesis and Analysis**
We differentiated parent, teacher, and youth self-report rat- ings; tools for clinicians; neuropsychological tests; biospeci- mens; EEG; and neuroimaging. We organized analyses according to prespecified outcome measures. A narrative overview summarized the range of diagnostic perfor- mance for key outcomes. Because lack of reported detail in many individual studies hindered use of meta-analytic models, we created summary figures to document the di- agnostic performance reported in each study. We used meta-regressions across studies to assess the effects of age, comorbidities, racial and ethnic composition, and di- agnostic setting (differentiating primary care, specialty care, school settings, mixed settings, and not reported) on diagnostic performance. One researcher with experience in use of specified standardized criteria initially as- sessed the overall strength of evidence (SoE) (see Supple- mental Appendix) for each study, then discussed it with the study team to communicate our confidence in each finding.

**RESULTS**
We screened 23 139 citations and 7534 publications re- trieved as full text against the eligibility criteria. In total, 231 studies reported in 290 publications met the eligibil- ity criteria (see [Fig].

Methodological quality of the studies varied. Selection bias was likely in two-thirds of studies; several were de- termined to be problematic in terms of reported study flow and timing of assessments (eg, not stating whether diagnosis was known before the results of the index test)and several lacked details on diagnosticians or diagnostic procedures (Supplemental Fig 1). Applicability concerns limited the generalizability of findings (Supplemental Fig 2), usually because youth with comorbidities were excluded. Many different tools were assessed within the broader cate- gories (eg, within neuropsychological tests), and even when reporting on the same diagnostic tool, studies often used different components of the tool (eg, different subscales of rating scales), or they combined components in a variety of ways (eg, across different neuropsychological test parameters).

The evidence table (Supplemental Table 10, Supplemental Appendix) shows each study’s finding. The follow- ing highlights key findings across studies.

**Parent Ratings**
Fifty-nine studies used parent ratings to diagnose ADHD ([Fig 2]. The most frequently evaluated tool was the CBCL (Child Behavior Checklist), alone or in combination with other tools, often using different score cutoffs for diagnosis, and evaluating different subscales (most frequently the at- tention deficit/hyperactivity problems subscale). Sensitivi- ties ranged from 38% (corresponding specificity 5 96%) to 100%. Area under the curve (AUC) for receiver operator char- acteristic curves ranged widely from 0.55 to 0.95 but 3 CBCL studies reported AUCs of 0.83 to 0.84. Few studies reported measurement of reliability. SoE was downgraded for study limitation (lack of detailed report- ing), imprecision (large performance variability), and in- consistent findings (Supplemental Table 1).

**Teacher Ratings**
Twenty-three studies used teacher ratings to diagnose ADHD (F[ig 2]. No 2 studies reported on rater agreement, internal consistency, or test-retest reliability for the same teacher rating scale. The highest sensitivity was 97% (specificity 5 26%). The Teacher Report Form, alone or in combination with Conners teacher rating scales, yielded sen- sitivities of 72% to 79% and specificities of 64% to 76%. reported AUCs ranged from 0.65 to 0.84. SoE was downgraded to low for imprecision (large performance variability) and inconsistency (results for specific tools not replicated), see Supplemental Table 2.

**Youth Self-Reports**
Six studies used youth self-reports to diagnose ADHD. No 2 studies used the same instrument. Sensitivities ranged from 53% (specificity 5 98%) to 86% (specificity 5 70%). AUCs ranged from 0.56 to 0.85. We down- graded SoE for domain inconsistency (only 1 study re- ported on a given tool and outcome), see Supplemental Table 3.

**Combined Rating Scales**
Thirteen studies assessed diagnostic performance of rat- ings combined across informants, often using machine learning for variable selection. Only 1 study compared per- formance of combined data to performance from single in- formants, finding negligible improvement (AUC youth 5 0.71; parent 5 0.85; combined 5 0.86). Other studies re- ported on limited outcome measures and used ad hoc meth- ods to combine information from multiple informants. The best AUC was reported by a machine learning supported study combining parent and teacher ratings (AUC 5 0.98).

**Additional Clinician Tools**
Twenty-four studies assessed additional tools, such as in- terview guides, that can be used by clinicians to aid diag- nosis of ADHD. Sensitivities varied, ranging from 67% (specificity 5 65%) to 98% (specificity 5 100%); specific-

ities ranged from 36% (sensitivity 5 89%) to 100% (sen- sitivity 5 98%). Some of the tools measured activity levels objectively using an actometer or commercially available activity tracker, either alone or as part of a diag- nostic test battery. Reported performance was variable (sensitivity range 25% to 100%, specificity range 66% to 100%, AUCs range 0.75–0.9996). SoE was downgraded for imprecision (large performance variability) and incon- sistency (outcomes and results not replicated), see Supple- mental Table 4.

**Neuropsychological Tests**
Seventy-four studies used measures from various neuro- psychological tests, including continuous performance tests (CPTs). Four of these included 3- and 4-year-old child- ren. A large majority used a CPT, which assessed omission errors (reflecting inattention), commission errors (impulsivity), and reaction time SD (response time variabil- ity). Studies varied in use of traditional visual CPTs, such as the Test of Variables of Attention, more novel, multifaceted “hybrid” CPT paradigms, and virtual reality CPTs built upon environments designed to emulate real-world classroom distractibility. Studies used idiosyncratic combinations of individual cognitive measures to achieve the best perfor- mance, though many reported on CPT attention and impul- sivity measures.

Sensitivity for all neuropsychological tests ranged from 22% (specificity 5 96%) to 100% (specificity 5 100%) [(Fig], though the latter study reported performance for unique composite measures without replication. Specific- ities ranged from 22% (sensitivity 5 91%) to 100% 0\.59 to 0.93. Sensitivity for all CPT studies ranged from 22% ( specificity 5 96) to 100% (specificity 5 75%). Spe- cificities for CPTs ranged from 22% (sensitivity 5 91%) to 100% (sensitivity 5 89%) ([Fig 3]. AUCs ranged from 0.59 to 0.93. SoE was deemed low for imprecise studies (large performance variability), see Supplemental Table 5.

**Biospecimen**

Seven studies assessed blood or urine biomarkers to di- agnose ADHD. These measured erythropoietin or eryth- ropoietin receptor, membrane potential ratio, micro RNA levels, or urine metabolites. Sensitivities ranged from 56% (specificity 5 95%) to 100% (specificity 5 100% for erythropoietin and erythropoietin receptors levels). Specificities ranged from 25% (sensitivity 5 79%) to 100% (sensitivity 5 100%). AUCs ranged from 0.68 to 1\.00.52 Little information was provided on reliability of markers or their combinations. SoE was downgraded for inconsistent and imprecise studies (Supplemental Table 6).

**EEG**
Forty-five studies used EEG markers to diagnose ADHD. EEG signals were obtained in a variety of patient states, even during neuropsychological test performance. Two- thirds used machine learning algorithms to select classifi- cation parameters. Several combined EEG with demo- graphic variables or rating scales. Sensitivity ranged widely from 46% to 100% (corresponding specificities 74 and 71%). One study that combined EEG with demo- graphics data supported by machine learning reported perfect sensitivity and specificity. Specificity was also variable and ranged from 38% (sensitivity 5 95%) to 100% (specificities 5 71% or 100%). Reported AUCs ranged from 0.63 to 1.0. SoE was downgraded for study imprecision (large performance variability) and limitations (diagnostic approaches poorly described), see Supplemental Table 7.

**Neuroimaging**
Nineteen studies used neuroimaging for diagnosis. One public data set (ADHD-200) produced several analyses. All but 2 used MRI: some functional MRI (fMRI), some structural, and some in combination, with or without magnetic resonance spectroscopy (2 used near-infrared spec- troscopy). Most employed machine learning to detect markers that optimized diagnostic classifications. Some combined im- aging measures with demographic or other clinical data in the prediction model. Sensitivities ranged from 42% (specificity 5 95%) to 99% (specificity 5 100%) using resting state fMRI and a complex machine learning algorithm to differentiate ADHD from neurotypical youth. Specificities ranged from 55% (sensitivity 5 95%) to 100% using resting state fMRI data. AUCs ranged from 0.58 to over 0.99, SoE was downgraded for imprecision (large performance variability) and study limitations (diagnostic models are often not well described, and the number and type of predictor variables entering the model were unclear). Studies generally did not validate diag- nostic algorithms or assess performance measures in an inde- pendent sample (Supplemental Table 8).

**Variation in Diagnostic Accuracy With Clinical Setting or Patient Subgroup**
Regression analyses indicated that setting was associated with both sensitivity (*P* 5 .03) and accuracy (*P* 5 .006)but not specificity (*P* 5 .68) or AUC (*P* 5 .28), with sensi- tivities lowest in primary care [(Fig]. Sensitivity, specif- icity, and accuracy were also lower when differentiating youth with ADHD from a clinical sample than from typi- cally developing youth (sensitivity *P* 5 .04, specificity *P* < .001, AUC *P* < .001) ([Fig 4], suggesting that clinical population is a source of heterogeneity in diagnostic performance.

Findings should be interpreted with caution, however, as they were not obtained in meta-analytic models and, consequently, do not take into account study size or quality.

Supplemental Figs 3–5 in the Supplemental Appendix document effects by age and gender. We did not detect statistically significant associations of age with sensitivity (*P* 5 .54) or specificity (*P* 5 .37), or associations of the proportion of girls with sensitivity (*P* 5 .63), specificity (*P* 5 .80), accuracy (*P* 5 .34), or AUC (*P* 5 .90).

**DISCUSSION**
We identified a large number of publications reporting on ADHD diagnostic tools. To our knowledge, no prior re- view of ADHD diagnostic tools has been as comprehen- sive in the range of tools, outcomes, participant ages, and publication years. Despite the large number of studies, we deemed the strength of evidence for the reported performance measures across all categories of diagnostic tools to be low because of large performance variability across studies and various limitations within and across studies.

**Measures for Diagnostic Performance**
We required that studies report diagnoses when using the tool compared with diagnoses made by expert mental health clinicians. Studies most commonly reported sensi- tivity (true-positive rate) and specificity (true-negative rate) when a study-specific diagnostic threshold was ap- plied to measures from the tool being assessed. Sensitiv- ity and specificity depend critically on that study-specific threshold, and their values are inherently a trade-off, such that varying the threshold to increase either sensi- tivity or specificity reduces the other. Interpreting diag- nostic performance in terms of sensitivity and specificity, and comparing those performance measures across stud- ies, is therefore challenging. Consequently, researchers more recently often report performance for sensitivity and specificity in terms of receiver operating characteris- tics (ROC) curves, a plot of sensitivity versus specificity across the entire range of possible diagnostic thresholds. The area under this ROC curve (AUC) provides an over- all, single index of performance that ranges from 0.5 (in- dicating that the tool provides no information above chance for classification) to 1.0 (indicating a perfect test that can correctly classify all participants as having ADHD and all non-ADHD participants as not having it). AUC values of 90 to 100 are commonly classified as ex- cellent performance; 80 to 90 as good; 70 to 80 as fair; 60 to 70 as poor; and 50 to 60 failed performance.

**Available Tools**
Most research is available on parental ratings. Overall, AUCs for parent rating scales ranged widely from “poor” to “excellent.” Analyses restricted to the CBCL, the most commonly evaluated scale, yielded more consistent “good” AUCs for differentiating youth with ADHD from others in clinical samples, but the number of studies contributing data were small. Internal consistency for rating scale items was generally high across most rating scales. Test-retest reliability was good, though only 2 studies reported it. One study reported moderate rater agreement between moth- ers and fathers for inattention, hyperactivity, and impulsiv- ity symptoms. Few studies included youth under 7 years of age.

AUCs for teacher rating scales ranged from “failed” to “good.” Internal consistency for scale items was gen- erally high. Teacher ratings demonstrated very low rater agreement with corresponding parent scales, suggesting either a problem with the instruments or a large variabil- ity in symptom presentation with environmental context (home or school).

Though data were limited, self-reports from youth seemed to perform less well than corresponding parent and teacher reports, with AUCs ranging from “failed” for CBCL or ASEBA when distinguishing ADHD from other patients to “good” for the SWAN in distinguishing ADHD from neurotypical controls.

Studies evaluating neuropsychological tests yielded AUCs ranging from “poor” to “excellent.” Many used idiosyncratic combinations of cognitive measures, which complicates interpretation of the results across studies. Nevertheless, extracting specific, comparable measures of inattention and impulsivity from CPTs yielded diagnostic performance ranging from “poor” to “excellent” in differen- tiating ADHD youth from neurotypical controls and “fair” in differentiating ADHD youth from other patients. No studies provided an independent replication of diagnosis using the same measure.

Blood biomarkers yielded AUCs ranging from “poor” (se- rum miRNAs) to “excellent” (erythropoietin and erythro- poietin receptors levels) in differentiating ADHD from neurotypical youth. None have been independently repli- cated, and test-retest reliability was not reported. Most EEG studies used machine learning for diagnostic classifi- cation. AUCs ranged from “poor” to “excellent” when dif- ferentiating ADHD youth from neurotypical controls. Diagnostic performance was not prospectively replicated in any independent samples.

Most neuroimaging studies relied on machine learning to develop diagnostic algorithms. AUCs ranged from “poor” to “excellent” for distinguishing ADHD youth from neurotypically developing controls. Most studies used pre-existing data sets or repositories to retrospec- tively discriminate youths with ADHD from neurotypical controls, not from other clinical populations and not pro- spectively, and none assessed test-retest reliability or the independent reproducibility of findings. Reporting of fi- nal mathematical models or algorithms for diagnosis was limited. Activity monitors have the advantage of provid- ing inexpensive, objective, easily obtained, and quantified measures that can potentially be widely disseminated and scaled.

Studies of combined approaches, such as integrating diagnostic tools with clinician impressions, were limited. One study reported increased sensitivity and specificity when an initial clinician diagnosis combined EEG indica- tors (the reference standard was a consensus diagnosis from a panel of ADHD experts). These findings were not independently replicated, however, and no test-retest reliability was reported.

**Importance of the Comparator Sample**
Many studies aimed to distinguish ADHD youth from neuro- typical controls, which is a distinction of limited clinical rele- vance. In clinically referred youth, most parents, teachers, and clinicians are reasonably confident that something is wrong, even if they are unsure whether the cause of their concern is ADHD. To be informed by a tool that the child is not typically developing is not particularly helpful. Moreover, we cannot know whether diagnostic performance for tools that discriminate ADHD youth only from neurotypical con- trols is determined by the presence of ADHD or by the pres- ence of any other characteristics that accompany clinical “caseness,” such as the presence of comorbid illnesses or symptoms shared or easily confused with those of other con- ditions, or the effects of chronic stress or current or past treatment. The clinically more relevant and difficult question is, therefore, how well the tool distinguishes youth with ADHD from those who have other emotional and behavioral problems. Consistent with these conceptual considerations that argue for assessing diagnostic performance in differenti- ating youth with ADHD from those with other clinical condi- tions, we found significant evidence that, across all studies, sensitivity, specificity, and AUC were all lower when differen- tiating youth with ADHD from a clinical sample than when differentiating them from neurotypical youth. These findings also suggest that the comparison population was a signifi- cant source of heterogeneity in diagnostic performance.

**Clinical Implications**
Despite the large number of studies on diagnostic tools, a valid and reliable diagnosis of ADHD ultimately still re- quires the judgement of a clinician who is experienced in the evaluation of youth with and without ADHD, along with the aid of standardized rating scales and input from multiple informants across multiple settings, including pa- rents, teachers, and youth themselves. Diagnostic tools per- form best when the clinical question is whether a youth has ADHD or is healthy and typically developing, rather than when the clinical question is whether a youth has ADHD or another mental health or behavioral problem. Diagnostic tools yield more false-positive and false-negative diagnoses of ADHD when differentiating youth with ADHD from youth with another mental health problem than when differentiat- ing them from neurotypically developing youth.

Scores for rating scales tended to correlate poorly across raters, and ADHD symptoms in the same child varied across settings, indicating that no single informant in a single setting is a gold-standard for diagnosis. There- fore, diagnosis using rating scales will likely benefit from a more complete representation of symptom expression across multiple informants (parents, school personnel, clinicians, and youth) across more than 1 setting (home, school, and clinic) to inform clinical judgement when making a diagnosis, thus, consistent with current guidelines. Unfortunately, methods for combining scores across raters and settings that improve diagnosis compared with scores from single raters have not been developed or prospectively replicated.

Despite the widespread use of neuropsychological testing to “diagnose” youth with ADHD, often at consid- erable expense, indirect comparisons of AUCs suggest that performance of neuropsychological test measures in diagnosing ADHD is comparable to the diagnostic performance of ADHD rating scales from a single infor- mant. Moreover, the diagnostic accuracy of parent rat- ing scales is typically better than neuropsychological test measures in head-to-head comparisons. Furthermore, the overall SoE for estimates of diagnostic performance with neuropsychological testing is low. Use of neuropsychological test measures of executive functioning, such as the CPT, may help inform a clinical diagnosis, but they are not definitive either in ruling in or ruling out a diagnosis of ADHD. The sole use of CPTs and other neuropsychological tests to diagnose ADHD, therefore, cannot be recommended. We note that this conclusion regarding diagnostic value is not relevant to any other clinical utility that testing may have.

No independent replication studies have been con- ducted to validate EEG, neuroimaging, or biospecimen to diagnose ADHD, and no clinical effectiveness studies have been conducted using these tools to diagnose ADHD in the real world. Thus, these tools do not seem remotely close to being ready for clinical application to aid diagno- sis, despite US Food and Drug Administration approval of 1 EEG measure as a purported diagnostic aid.

**Future Research**
All studies of diagnostic tools should report data in more detail (ie, clearly report false-positive and -negative rates, the diagnostic thresholds used, and any data manipula- tion undertaken to achieve the result) to support meta- analytic methods. Studies should include ROC analyses to support comparisons of test performance across studies that are independent of the diagnostic threshold applied to measures from the tool. They should also include as- sessment of test-retest reliability to help discern whether variability in measures and test performance is a func- tion of setting or of measurement variability over time. Future studies should address the influence of co-occurring disorders on diagnostic performance and how well the tools distinguish youth with ADHD from youth with other emo- tional and behavioral problems, not simply from healthy controls. More studies should compare the diagnostic accu- racy of different test modalities, head-to-head. Independent, prospective replication of performance measures of diagnos- tic tools in real-world settings is essential before US Food and Drug Administration approval and before recommenda- tions for widespread clinical use.

Research is needed to identify consensus algorithms that combine rating scale data from multiple informants to im- prove the clinical diagnosis of ADHD, which at present is of- ten unguided, ad hoc, and suboptimal. Diagnostic studies using EEG, neuroimaging, and neuropsychological tests should report precise operational definitions and measure- ments of the variable(s) used for diagnosis, any diagnostic algorithm employed, the selected statistical cut-offs, and the number of false-positives and false-negatives the diagnostic tool yields to support future efforts at synthetic analyses.

**CONCLUSIONS**
Objective, quantitative neuropsychological test measures of executive functioning correlate only weakly with the clinical symptoms that define ADHD. Thus, many youth with ADHD have normal executive functioning profiles on neuropsychological testing, and many who have im- paired executive functioning on testing do not have ADHD. Future research is needed to understand how test measures of executive functioning and the real-world functional problems that define ADHD map on to one an- other and how that mapping can be improved.

One of the most important potential uses of systematic reviews and meta-analyses in improving the clinical diagno- sis of ADHD and treatment planning would be identification of effect modifiers for the performance of diagnostic tools: determining, for example, whether tools perform better in patients who are younger or older, in ethnic minorities, or those experiencing material hardship, or who have a co- morbid illness or specific ADHD presentation. Future studies of ADHD should more systematically address the modifier effects of these patient characteristics. They should make available in public repositories the raw, individual- level data and the algorithms or computer code that will aid future efforts at replication, synthesis, and new discov- ery for diagnostic tools across data sets and studies.

Finally, no studies meeting our inclusion criteria assessed the consequences of being misdiagnosed or labeled as either having or not having ADHD, the diagnosis of ADHD specifi- cally in preschool-aged children, or the potential adverse consequences of youth being incorrectly diagnosed with or without ADHD. This work is urgently needed.